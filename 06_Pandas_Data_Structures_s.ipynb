{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 Pandas Data Structures\n",
    "File(s) needed: stats_exam_3_scores.csv, Applewood_2011.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two primary data structures in pandas are the **Series** and **DataFrame**. We've already seen the DataFrame in action but we will spend more time getting to know these two structures because what we will do later depends on our understanding them.\n",
    "\n",
    "- Pandas DataFrame\n",
    "    - A rectangular dataset, like a table\n",
    "    - Has columns and rows of data\n",
    "    - Can hold heterogeneous data between columns\n",
    "    - Within a column, must have the same data type\n",
    "- Pandas Series\n",
    "    - A \"column\" of data\n",
    "    - DataFrames are composed of Series objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make sure pandas library is loaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pandas Series\n",
    "The pandas series is a one-dimensional array of indexed data. It can be created from a list as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can create it directly or in parts.\n",
    "# Here we create the list first.\n",
    "my_list = [2.25, 2.5, 2.75, 3.0]\n",
    "my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the series from the list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see both a sequence of the values we used to create our series and a sequence of indices (the row numbers on the left). We can call the `.values` and/or `.index` attributes if we need access to either of them individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display the data values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display the index values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the square bracket notation to access individual values using the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# access the value at index 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples may make the pandas Series look like a Python list or simple numPy array. But there is a big difference.\n",
    "- Implicitly defined index - list or array objects use implicitly defined integer indeces to access values.\n",
    "- Explicitly defined index - the pandas Series object has an explicitly defined index we use to access the values. \n",
    "    - That may not seem like much of a difference, but the result is that **_the index can be values of any type we want._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add letter index values and retrieve value at index C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can even use nonsequential values for the indices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `loc` and `iloc` to slice a series. Remember that we use `loc` with the index value and `iloc` with the integer position value. (The square brackets only work with the index value.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display what is in the data object as a reminder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show the value at index 37\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show the value at position 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show just the values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many methods built into the pandas series we can use to perform common calculations. We will talk more about these as we need them, but here is an example of calculating the mean.\n",
    "\n",
    "See https://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html#descriptive-statistics for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculating the mean of the data series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean subsetting in a series\n",
    "We have already worked with ways to get a subset of our data based upon index values or positions. We will seldom (if ever) know the exact index or position values we want. Instead, we typically look for rows that meet some condition, as with a SQL query or an Excel formula. We can use a simple conditional statement that evaluates to TRUE or FALSE to get the rows of interest.\n",
    "\n",
    "We will use the square brackets format to select our subset, so the general form of this statement is\n",
    "```\n",
    "series_name[conditional statement]\n",
    "```\n",
    "\n",
    "We need a little bigger data set for this example. Load \"stats_exam_3_scores.csv\" and convert the results to a series to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First, let's load a bigger set of data into a series.\n",
    "# Read the file \"stats_exam_3_scores.csv\" and get it into a Series named \"scores\"\n",
    "# pd.read_csv saves data in a data frame so we will extract the scores column to a Series.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract the data to a series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the scores series to see the data we loaded.\n",
    "\n",
    "# Use the describe method to see what the data looks like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, subset the data in the series using a Boolean statement.\n",
    "\n",
    "In this example, we want to get all the rows with scores of 145 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Subset all the rows with scores of 145 or more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is actually happening in this statement? Let's run just the conditional part to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use print() to see just the conditional results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like a Boolean series is generated and subsequently applied to subset our data.\n",
    "\n",
    "When we wrote the statement `data['C']` earlier, we told pandas to return the row where the index value was a capital C. This time, we effectively told pandas to return the rows where the value was equal or greater to 145.\n",
    "\n",
    "Specifically, the statement `scores[scores >= 145]` tells pandas to do the following:\n",
    "- look at each value in `scores`\n",
    "- create a parallel Boolean series that contains the value `TRUE` if the value is greater than or equal to 145 or `FALSE` if it is not.\n",
    "- subset all the rows that have a `TRUE` value in the corresponding position in the parallel Boolean series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Try another one: inspect all the failing scores (below 60 percent)\n",
    "#    That is a score of 90 out of the 150 total points on this exam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait a minute .... where's the loop?\n",
    "If we were doing this same task in base Python (or any other typical programming language), we would loop through the data one element at a time. We would compare each value to our conditional statement and include it in the results if the test was true and omit it if it were false.\n",
    "\n",
    "Many of the methods we will use for the pandas series and data frame are **_vectorized_**. That means they work on the entire data structure at once. Think of it this way - it's like the loop is built into the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# One more: save all the values above 80% (score of 120) to a new series called \"above_80\"\n",
    "# Start with the general form of this type of statement.\n",
    "# TIP: use the quantile() method of the pandas series in the conditional statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pandas DataFrame\n",
    "As we saw previously, the DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a Python dictionary of \"aligned\" Series objects. \"Aligned\" means they share the same index. \n",
    "\n",
    "It is generally the most commonly used pandas object. You will soon see why.\n",
    "\n",
    "Along with the data, you can optionally pass _index_ (row labels) and _columns_ (column labels) arguments. If you pass an index and/or columns, you are specifying the index and/or column names of the resulting DataFrame.\n",
    "\n",
    "If row and column labels are not passed, they will be constructed by pandas from the input data based on common sense rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example: create a DataFrame using dictionaries\n",
    "# Python dictionaries are made up of a key and a value. In this case, the values are themselves lists and\n",
    "# the key is used as the column name.\n",
    "\n",
    "# You are very unlikely to use this method in practice, so don't get bogged down by the details.\n",
    "pythons = pd.DataFrame({\n",
    "    'Name':['Michael', 'John', 'Graham', 'Eric'],\n",
    "    'Office':['COB 305M','COB 305J','COB 305G','COB 305E'],\n",
    "    'Age':[59, 66, 70,72],\n",
    "    'Favorite':['Finland','Ministry of Silly Walks',\"What's All This Then?\",'The Galaxy Song']})\n",
    "pythons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What part of this table is the actual data?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default we get integer row index values. What if we wanted to use the values in our 'Name' column as the row labels? Move those values to the index parameter. We still maintain the integer position values for each row but now can specify a particular index name (or label) for each row.\n",
    "\n",
    "Kind of confusing, so let's look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Specify row names using the index parameter.\n",
    "# Add the columns parameter to specify a particular column order when we create the dataframe.\n",
    "pythons = pd.DataFrame(\n",
    "    data = {'Office':['COB 305M','COB 305J','COB 305G','COB 305E'],\n",
    "            'Age':[59, 66, 70,72],\n",
    "            'Favorite':['Cheese Shop','Ministry of Silly Walks',\"What's All This Then?\",'Wink wink nudge nudge']},\n",
    "    index = ['Michael', 'John', 'Graham', 'Eric'],\n",
    "    columns = ['Age','Favorite','Office'])\n",
    "pythons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What part of _this_ table is the actual data? What has changed? \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# What are the index values?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get a row by index value (label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get a row by index position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We typically have our data table constructed so each row is an instance of an entity and each column is an attribute of the entity. In this case, our data is modeling Monty Python members (the entity), with each row being one of the members' data and each column is some descriptive attribute of the members.\n",
    "\n",
    "The data frame is actually built from the columns, not the rows, however. Each column is a series of the same data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see this, look at individual columns and how they all have the same row names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Subset the Age column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Subset the Favorite column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# What data type is pythons_fav?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the Series object, the pandas DataFrame object has `index` and `columns` attributes we can use to get the index labels and column labels, respectively. We can also use the `values` attribute to see the underlying data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# What are the index values?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the data values in the data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame maps a column name to the Series object that contains the column's data. For our purposes, we will look at it the way we look at tables of data.\n",
    "\n",
    "If we reference a particular column, we can see the contents of that column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access a single row by giving the `values` attribute a row index number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use values attribute to get row with index 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# remember using the column name?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to refer to a row by the key, we have to use **_slicing_**. With slicing, we are trying to select a subset of the rows in the data.\n",
    "\n",
    "Note that when we use key values, the operation is right INclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example: slicing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can also use row numbers in a slice\n",
    "# With the row numbers it is right EXclusive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we did with the series, we can also use Boolean statements to select rows that meet a condition. We will talk about this more as we need it, but think of it as being able to run a simple query on the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use a conditional to select rows where Age > 67.\n",
    "# You will need to use dot notation to reference the column name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do another example with a bigger data set. Load the \"Applewood_2011.csv\" data into a data frame called \"cars.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the \"Applewood_2011.csv\" data into \"cars\" object and make sure it is what we expect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get rows where age > 67 and save as sub_cars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get rows where the location is Olean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get rows for customers who made at least 3 previous purchases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding rows\n",
    "It is usually easier to add rows in the original data set and reimport the file. However, we can add a row to an existing data frame by first creating the new row as a data frame with the same structure as our original data frame. Then we use the `append()` method of the original data frame object to add the new row.\n",
    "\n",
    "Let's do an example so you can see why you don't want to do it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add a row for Terry Jones: 63 years old, favorite is \"Spam\", and office is \"COB 305T\"\n",
    "# First, create the new data as a data frame using a dictionary.\n",
    "new_row = pd.DataFrame(\n",
    "    data = [{'Favorite':'Spam','Age':63,'Office':'COB 305T'}],\n",
    "    index= ['Terry'])\n",
    "new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add the series to the data frame using the append() method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show the index values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding columns\n",
    "There are many reasons we might add a column. When we add a column, pandas matches index key values to make sure the data stays aligned. Let's add a column to our pythons data frame to see how this works.\n",
    "\n",
    "First we need a column (i.e., another series) to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create another series of data related to the pythons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remember what the pythons data frame looks like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add the new column by specifying the new column name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if an index value doesn't match up? Go back to the cell creating the new column and change a value, then rerun these cells to see the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping columns\n",
    "We may decide we don't need one or more columns in our data set any more. If that is the case, we can use the `drop()` method to remove the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop the Shoe_size column from pythons and see the change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the data frame. What's the deal?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a failsafe, the results of the drop method are not saved to the original data frame by default. There are two options\n",
    "1. save the results to a new data frame object\n",
    "2. use the parameter `inplace=True` to commit the change to the original data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add inplace=True to save the changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop multiple columns at once, pass the column names as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop Location and Previous from the Applewood data frame\n",
    "\n",
    "\n",
    "# Once you like the results, save to another object or add inplace parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More about data file access\n",
    "NOTE: This is the situation where you can benefit from having a folder in your project structure called **working** (or something similar) to hold modified data files. That allows you to keep original data and modified data separate.\n",
    "\n",
    "---\n",
    "#### Pickling\n",
    "There are times when you may want to save your modified data frame to disk. One way to do that is by **_pickling_**. Pickling is how Python saves a data container as a binary file.\n",
    "\n",
    "This may come in handy if you need to save your data results from one program for use in another. However, the data can only be accessed with Python. Other programs will not be able to open it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save a data frame to a pickled file - specify a path and file name\n",
    "# The filename extension doesn't matter, but \"pickle\" is self-documenting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read a pickled file and save it in a named object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV files\n",
    "Comma-separated value (CSV) files are a very flexible type to use for data storage. They are simply basic text files with commas between each row element and any program can open them. We have already used the basic version of `pd.read_csv` to load some of our data. The DataFrame and Series objects have a `to_csv()` method built in that allow you to save data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save a data frame as a csv file and open it in Excel or Notepad to see what is there\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save the data frame without row indexes\n",
    "# CAUTION: the row indexes might be important so only use this if they are not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excel files\n",
    "<p style=\"font-size:125%;padding:35px;text-align:center;background-color:dodgerblue;color:white\">You should stick with CSV files as much as possible. If you have data in an Excel spreadsheet, you will usually be better off if you use Excel to save it in CSV format and then work with the CSV file.</p>\n",
    "\n",
    "However, pandas can read Excel files directly. The following is provided for your reference.\n",
    "\n",
    "The `read_excel()` method in pandas can read Excel 2003 (.xls) and Excel 2007+ (.xlsx) files using the xlrd Python module. The `to_excel()` instance method is used for saving a DataFrame to Excel. Generally the semantics are similar to working with csv data.\n",
    "\n",
    "In the most basic use-case, `read_excel` takes a path to an Excel file, and the sheet_name indicating which sheet to parse. You can also specify a column to use as the index values and how to handle missing data.\n",
    "\n",
    "```\n",
    "# Returns a DataFrame\n",
    "read_excel('path_to_file.xls', sheet_name='Sheet1')\n",
    "```\n",
    "\n",
    "Using more of the available arguments:\n",
    "```\n",
    "Using the sheet name, specifying the index, and missing values as NA:\n",
    "  read_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])\n",
    "\n",
    "Using the sheet index, specifying the index, and missing values as NA:\n",
    "  read_excel('path_to_file.xls', 0, index_col=None, na_values=['NA'])\n",
    "\n",
    "Using all default values:\n",
    "  read_excel('path_to_file.xls')\n",
    "\n",
    "Using None to get all sheets:\n",
    "  read_excel('path_to_file.xls', sheet_name=None)\n",
    "\n",
    "Using a list to get multiple sheets:\n",
    "Returns the 1st and 4th sheet, as a dictionary of DataFrames.\n",
    "  read_excel('path_to_file.xls', sheet_name=['Sheet1', 3])\n",
    "```\n",
    "\n",
    "As you see in the last two entries, `read_excel` can read more than one sheet at a time by setting sheet_name to either a list of sheet names, a list of sheet positions, or None to read all sheets. Sheets can be specified by sheet index or sheet name, using an integer or string, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database files\n",
    "pandas also has built-in capabilities for reading database files. Many of the best known databases rely on SQL and pandas leverages SQL for data access. The library `sqlalchemy` provides these functions for SQL databases like SQLite, MySQL, SQL Server, and MS Access (through the ODBC standard). We will not have time to cover these in class but you should be aware of them for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
